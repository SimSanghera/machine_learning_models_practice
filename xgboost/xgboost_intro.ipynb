{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to XGBoost in Python  \n",
    "  \n",
    "This is just a notebook taking tutorials and information from multiple websites to familiarise myself with XGBoost machine learning algorithms.  \n",
    "  \n",
    "### What is XGBoost?  \n",
    " - XGBoost is a machine learning algorithm that belongs to the _ensemble_ learning category  \n",
    " - specifically the gradient boosting group  \n",
    " - it utilises decision trees as base learners and employs regularisation techniques to enhance model generalisation  \n",
    " - ensemble methods, or boosting, create a sequence of models that correct the mistakes of models before them in the sequence  \n",
    " - key components: \n",
    "   - _decision trees_  \n",
    "   - _objective functions_  \n",
    "   - _learning tasks_  \n",
    " - features: variables used to predict the target variable  \n",
    " - important to know which feature has more predictive power  \n",
    " - \n",
    "\n",
    "\n",
    "#-----\n",
    "   - **decision trees**\n",
    "     - are supervised learning algorithms\n",
    "     - can create both classification and regression models  \n",
    "     - looks like a flow charts, start at the _root node_ with a specific question of data\n",
    "     - then leads to branches holding potential answers  \n",
    "     - branches lead to _decision (internal) nodes_, which ask more questions that lead to more outcomes  \n",
    "     - this continues until the data reaches the _terminal node_ (leaf)  \n",
    "     - benefits: lay out the problem and all outcomes; analyses possible consequences of a decision; can predict outcomes for future data  \n",
    "     - types: classification trees; regression trees\n",
    "   - **regularization** \n",
    "     - reduces overfitting in machine learning models  \n",
    "     - methods: _lasso_; _ridge_; _elastic net_  \n",
    "     - _lambda_ - this is a crucial parameter chosen for cross-validation to balance fitting the training data  \n",
    "     - how does it work:\n",
    "       - adds a penalty term to the standard loss function  \n",
    "       - encourages the model to keep parameters small  \n",
    "       - modifies the loss function (Regularise Loss = Original Loss + lambda x Penalty)\n",
    "     - _lasso_: penalty is the sum of the absolute values of the parameters  \n",
    "     - _ridge_: penalty is the sum of the squares of the parameters  \n",
    "     - why: prevent overfitting; improve model generalisation (through lack of overfitting); handle multicolinearity; feature selection; improve robustness to noise; trade bias for variance; aid in convergence  \n",
    "----- \n",
    "\n",
    "- XGBoost builds a predictive model by combining the predictions of multiple individual models in an iterative manner  \n",
    "- The algorithms works by sequentially adding weak learners to the ensemble  \n",
    "  - each new learner focusing on correcting the errors made by existing ones  \n",
    "- It uses a gradient descent optimisation technique to minimise a predefined loss function during training  \n",
    "- **Bagging**  \n",
    "  - sample randomly from the initial sample data to obtain the sample subset  \n",
    "  - reduces variance by averaging predictions from models trained on different subsets of data  \n",
    "- **Boosting**  \n",
    "  - trees built sequentially so each subsequent tree aims to reduce the errors of the previous tree  \n",
    "  - each tree learns from its predecessor & updates the residual errors  \n",
    "  - base learners in boosting are weak learners - bias is high, predictive power is low  \n",
    "  - makes use of trees with fewer splits  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Framework  \n",
    "https://medium.com/@cristianleo120/the-math-behind-xgboost-3068c78aad9d  \n",
    "  \n",
    "1. Initialisation  \n",
    " - start with a base model: simple prediction for all instances (e.g. mean for regression, mode for classification)  \n",
    " - initial prediction: algorithm will iteratively improve on it  \n",
    "  \n",
    "2. Iterative improvement  \n",
    " - sequentially add weak learners\n",
    " - residuals/error calculation: calculate after each tree is added  \n",
    "\n",
    "3. Gradient Descent Step  \n",
    " - compute negative gradient: the neg gradient of the loss function, showing how the prediction should be changed to reduce loss  \n",
    " - fit new model to gradient  \n",
    "\n",
    "4. Update model with Learning Rate  \n",
    " - apply learning rate: predictions of new tree scaled by parameter known as _learning rate_ (_shrinkage factor_)  \n",
    " - update the model  \n",
    " - learning rate controls how fast the model learns and helps prevent overfitting  \n",
    "\n",
    "5. Regularisation  \n",
    " - control model complexity: regularise the model\n",
    " - e.g. set maximum depth for trees, minimum samples for a leaf, number of trees  \n",
    "\n",
    "6. Stopping Criteria  \n",
    " - determine when to stop: stop after either fixed number of trees are added, or when improvement drops below a threshold  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation: Starting Model  \n",
    "  \n",
    " - this step involves creating a starting model that provides a baseline prediction for all instances in the dataset  \n",
    " - regression tasks: initial model often predicts the mean  \n",
    " - classification tasks: intial model often predicts the mode or log odds ratio  \n",
    "\n",
    "\n",
    "## Iterative Improvement  \n",
    "  \n",
    " - the algorithm successively adds weak learners (usually decision trees) to the ensemble  \n",
    " - targets the shortcomings of the existing combined model  \n",
    " - each new learner focuses on the errors or residuals  \n",
    " - **How**  \n",
    "   - _calculate residuals/error_  \n",
    "     - regression residuals = differences between observed and predicted values  \n",
    "     - classification residuals = derivative of the loss function with respect to the current model's predictions  \n",
    "   - _fit a weak learner to residuals_  \n",
    "     - the new learner is trained on the residuals  \n",
    "     - aims to predict residuals for each instance  \n",
    "   - _compute outcome of the learner_  \n",
    "     - regression = predicted residual  \n",
    "     - classification = calculate value to update current model to improve accuracy  \n",
    "   - _update model_  \n",
    "     - model is updated by adding the scaled output of the new learner to the existing prediction  \n",
    "   - _repeat steps_  \n",
    "     - repeated for specified number of iterations, or until convergence criterion is met  \n",
    "\n",
    "  \n",
    "## Gradient Descent Step  \n",
    "\n",
    " - gradient boosting minimises the loss function by moving in the direction of the steepest descent as defined by the negative gradient  \n",
    " - occurs for each stage in the boosting process  \n",
    " - _calculate gradient_  \n",
    " - _fit a weak learner to the negative gradients_  \n",
    " - _determine step size (learning rate)_ - how much the model is adjusted by  \n",
    " - _update the model_  \n",
    "  \n",
    "\n",
    "## Update Model with Learning Rate  \n",
    "  \n",
    " - predictions from newly added learner are incorporated into existing model  \n",
    " - _apply learning rate_  \n",
    " - _combine weak learner's predictions with current model_  \n",
    "\n",
    "  \n",
    "## Regularisation  \n",
    " - adds constraints or penalties to the model  \n",
    " - controls the complexities  \n",
    " - helps make it less sensitive to noise in the training data  \n",
    " - _tree constraints_:  \n",
    "   - max depth - limit depth of each tree; deeper trees can capture more complex patterns but might overfit  \n",
    "   - min samples per leaf - prevents model from learning rules that are too specific to the training data  \n",
    "   - max leaves  \n",
    " - _shrinkage/learning rate_  \n",
    "   - when _v_ is less than 1, it reduces the contribution of each tree and slows learning \n",
    "   - helps generalisation  \n",
    " - _subsampling of data (stochastic gradient boosting)_  \n",
    "   - feature subsampling - using subset of features for fitting each tree (RF approach)  \n",
    "   - data subsampling - using a random subset of training data to fit each tree = bagging  \n",
    " - _penalty on leaf weights_  \n",
    "   - LASSO - adds penalty to absolute value of magnitude of coefficients; can let some feature weights be 0  \n",
    "   - RIDGE - add penalty equal to square of the magnitude of the coefficients; stops large weights, but does not typically have 0 weights  \n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
